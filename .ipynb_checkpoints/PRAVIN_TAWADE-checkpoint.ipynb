{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyiGOxJ1uQC4"
   },
   "source": [
    "#  Identifying Entities in Healthcare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KerpgR4TuQC8"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWp8G9NmuQC8"
   },
   "source": [
    "Now, let’s consider a hypothetical example of a health tech company called ‘BeHealthy’. Suppose ‘BeHealthy’ aims to connect the medical communities with millions of patients across the country. \n",
    "\n",
    " \n",
    "\n",
    "‘BeHealthy’ has a web platform that allows doctors to list their services and manage patient interactions and provides services for patients such as booking interactions with doctors and ordering medicines online. Here, doctors can easily organise appointments, track past medical records and provide e-prescriptions.\n",
    "\n",
    " \n",
    "\n",
    "So, companies like ‘BeHealthy’ are providing medical services, prescriptions and online consultations and generating huge data day by day.\n",
    "\n",
    " \n",
    "\n",
    "Let’s take a look at the following snippet of medical data that may be generated when a doctor is writing notes to his/her patient or as a review of a therapy that he or she has done.\n",
    "\n",
    " \n",
    "\n",
    "“The patient was a 62-year-old man with squamous cell lung cancer, which was first successfully treated by a combination of radiation therapy and chemotherapy.”\n",
    "\n",
    " \n",
    "\n",
    "As you can see in this text, a person with a non-medical background cannot understand the various medical terms. We have taken a simple sentence from a medical data set to understand the problem and where you can understand the terms ‘cancer’ and ‘chemotherapy’. \n",
    "\n",
    " \n",
    "\n",
    "Suppose you have been given such a data set in which a lot of text is written related to the medical domain. As you can see in the dataset, there are a lot of diseases that can be mentioned in the entire dataset and their related treatments are also mentioned implicitly in the text, which you saw in the aforementioned example that the disease mentioned is cancer and its treatment can be identified as chemotherapy using the sentence.\n",
    "\n",
    " \n",
    "\n",
    "But, note that it is not explicitly mentioned in the dataset about the diseases and their treatment, but somehow, you can build an algorithm to map the diseases and their respective treatment.\n",
    "\n",
    " \n",
    "\n",
    "Suppose you have been asked to determine the disease name and its probable treatment from the dataset and list it out in the form of a table or a dictionary like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwhwNhY9mAQT"
   },
   "source": [
    "<p>\n",
    "<img src =\"https://images.upgrad.com/0891d77b-b9ca-4e9d-8934-d9a9b078a51c-syntactic%20sol%20pic1.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 1.</b> </center> \n",
    " </br>  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NA7X1e4uQC9"
   },
   "source": [
    "# Business Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrMi6iv7uQC9"
   },
   "source": [
    "BeHealthy require **predictive model** which can **identify disease and treatment** from the patients interaction with doctor or ordering medicines online.\n",
    "\n",
    "By observing the requirement, it is clearly visible that we have to process the textual sentence and identify the entities like Disease and Treatment. We can predict these all requirements using\n",
    "  -  CRF (Conditional Random Field) classifier\n",
    "  -  HMM (Hidden Markov Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDWGz2juQC-"
   },
   "source": [
    "# IMPORT LIBRARIES AND DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ4Qadm3vepJ",
    "outputId": "a2d9586a-9b19-4297-c56b-880a936832da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycrf\n",
      "  Downloading pycrf-0.0.1.tar.gz (1.1 kB)\n",
      "Building wheels for collected packages: pycrf\n",
      "  Building wheel for pycrf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycrf: filename=pycrf-0.0.1-py3-none-any.whl size=1897 sha256=16c9c92eb32413f379e67672c0b43f69da9d3ad03070af0033eac2fe268f6dcf\n",
      "  Stored in directory: /Users/tawadpra/Library/Caches/pip/wheels/03/bf/ca/6777c01db8f2183ae7c5fadfc62d6e88d3e6d600c6379fa3c9\n",
      "Successfully built pycrf\n",
      "Installing collected packages: pycrf\n",
      "Successfully installed pycrf-0.0.1\n",
      "Collecting sklearn-crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from sklearn-crfsuite) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from sklearn-crfsuite) (4.50.2)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.9-cp38-cp38-macosx_10_9_x86_64.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tabulate, python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pycrf\n",
    "!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools\n",
      "  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel\n",
      "  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip, setuptools, wheel\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 50.3.1.post20201107\n",
      "    Uninstalling setuptools-50.3.1.post20201107:\n",
      "      Successfully uninstalled setuptools-50.3.1.post20201107\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.35.1\n",
      "    Uninstalling wheel-0.35.1:\n",
      "      Successfully uninstalled wheel-0.35.1\n",
      "Successfully installed pip-23.0.1 setuptools-67.6.0 wheel-0.40.0\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.1-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp38-cp38-macosx_10_9_x86_64.whl (490 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.0/490.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-macosx_10_9_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp38-cp38-macosx_10_9_x86_64.whl (848 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.1/848.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.6-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: cymem, wasabi, typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, catalogue, blis, srsly, pydantic, preshed, pathy, confection, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.6 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n",
      "2023-03-21 20:51:03.859818: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (20.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.50.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.19.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFTS1hy6uQC-",
    "outputId": "af56a499-6308-4567-8a6d-1d82482bede0"
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np                                                 # Implemennts milti-dimensional array and matrices\n",
    "import pandas as pd                                                # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt                                    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\n",
    "import seaborn as sns                                              # Provides a high level interface for drawing attractive and informative statistical graphics\n",
    "\n",
    "import re                                                          # Regular expressions \n",
    "import spacy                                                       #  NLP, POS tag check\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qvMw9QSnuQDA"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF4ETWBCuQDA"
   },
   "source": [
    "## Data Loading and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHyDFBPvuQDB"
   },
   "source": [
    "- **Data Set** - It contains four data file for this activity to proceed, they are\n",
    "  - Train Sentence Dataset\n",
    "  - Train Label Dataset\n",
    "  - Test Sentence Dataset\n",
    "  - Test Label Dataset\n",
    "\n",
    "Sentence file contains all interations between patients and doctor and Label file contains all enitiy tags for particular words arranged as per sentence. We need to do few preprocessing while accessing dataset we will explore that in further steps.\n",
    "\n",
    "We have the train and the test datasets; the train dataset is used to train the CRF model, and the test dataset is used to evaluate the built model.\n",
    "\n",
    "Let’s take a look at the structure of these datasets using the image provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSt04B3Fmtxm"
   },
   "source": [
    "<p>\n",
    "<img src =\"https://images.upgrad.com/af3536e2-c88f-42f1-8dda-fa563763ecff-Syntactic%20sol%20pic2.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 2.</b> </center> \n",
    " </br>  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7uPGU6zm6K8"
   },
   "source": [
    "Here, we need to understand that each word in this dataset is provided in a single line. So, first, we need to club all these words together to form the sentences. Moreover, there are blank lines given in the dataset that have been highlighted in the image given above. These blank lines indicate that a new sentence is starting from the next line onwards to the next blank line.\n",
    "\n",
    "In the image provided above, you need to make the sentences in the following way:\n",
    "\n",
    "Sentence1: …using a Spearman-rank Correlation\n",
    "Sentence2: This relationship should be taken into account when interpreting the AFI as a measure of fetal well-being.\n",
    "Sentence3: The study population…\n",
    "...and so on.\n",
    "\n",
    "\n",
    "We can also refer to the image given below to get a better idea on how to create sentences from words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivw9T3gynDQN"
   },
   "source": [
    "<p>\n",
    "<img src =\"https://images.upgrad.com/2a1ec8a4-e26c-4b5b-bfe0-1816d14a4a30-syntactic%20sol%20pic3.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 3.</b> </center> \n",
    " </br>  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNiVTP17nSWG"
   },
   "source": [
    "In this ‘train_sent’ dataset, there are a total of **2,599** sentences when you form the sentences from the words. Similarly, there are a total of **1,056** sentences in the ‘test_sent’ dataset when you form the sentences from the words.\n",
    "\n",
    "Now, let’s take a look at the next datasets that are named ‘train_label’ and ‘test_label’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpfJ4uySnX3M"
   },
   "source": [
    "<p>\n",
    "<img src =\"https://images.upgrad.com/bdd7f8f5-0fbb-4b46-9c2c-500a68c40d2e-syntactic%20sol%20pic4.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 4.</b> </center> \n",
    " </br>  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdh6a4e4ng8e"
   },
   "source": [
    "The above dataset is about the labels corresponding to the diseases and the treatment. There are three labels that have been used in this dataset: O, D and T, which are corresponding to ‘Other’, ‘Disease’ and ‘Treatment’, respectively.\n",
    "\n",
    " \n",
    "\n",
    "These labels correspond to each word that is available in the ‘train_sent’ and 'test_sent' datasets. So, there is one-to-one mapping of each label available in the 'train_label' and 'test_label' datasets with the words that are available in the 'train_sent' and 'test_sent' datasets, respectively. \n",
    "\n",
    "We need to again create the lines of labels corresponding to each sentence in the ‘train_sent’ and the ‘test_sent’ datasets as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYMW6UGmnmtS"
   },
   "source": [
    "<p>\n",
    "<img src =\"https://images.upgrad.com/cad61c6d-534f-4452-81e6-dc02d7c8fcde-syntactic%20sol%20pic5.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 5.</b> </center> \n",
    " </br>  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9SmfdUInuDv"
   },
   "source": [
    "So, in this ‘train_label’ dataset, there are a total of **2,599** lines of labels when you form the lines from the label dataset. Similarly, there are a total of **1,056** lines of labels in the ‘test_label’ dataset when you form the lines from the label dataset.\n",
    "\n",
    " \n",
    "\n",
    "In this assignment, you need to perform the following broad steps:\n",
    "\n",
    "- We need to process and modify the data into sentence format. This step has to be done for the 'train_sent' and ‘train_label’ datasets and for test datasets as well.\n",
    "-  After that, we need to define the features to build the CRF model.\n",
    "- Then, you need to apply these features in each sentence of the train and the test dataset to get the feature values.\n",
    "- Once the features are computed, you need to define the target variable and then build the CRF model.\n",
    "- Then, we need to perform the evaluation using a test data set.\n",
    "- After that, we need to create a dictionary in which diseases are keys and treatments are values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QRrmcesv631"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs7KT89KwNvc"
   },
   "source": [
    "## Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuuk0pvwUnm"
   },
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qQEgS8_IwWIB"
   },
   "outputs": [],
   "source": [
    "# Extract sentence from words\n",
    "def content_extract(file_path='',sep='\\t'):\n",
    "    '''It helps to extract the word based on the separator to form the sentence'''\n",
    "    try:\n",
    "        with open (file_path,'r',encoding='utf-8') as text:\n",
    "            if text.mode  == 'r':\n",
    "                content = text.readlines()\n",
    "        sentence = []\n",
    "        final_sentence=''\n",
    "        for c in content:\n",
    "            content_word = c.strip('\\n')\n",
    "            if content_word == '':\n",
    "                #Once it get matched with separator, it appends previous extracted concatenated string as sentence\n",
    "#                 final_sentence = re.sub('(?<=[\\(]) | (?=[%\\',)])','', final_sentence)\n",
    "                sentence.append(final_sentence.strip(' '))\n",
    "\n",
    "                #Initialize for next sentence\n",
    "                final_sentence=''\n",
    "            else:\n",
    "                # Till the loop identifies the separator it concatenates string\n",
    "                final_sentence+=content_word+' '\n",
    "        print('Total identified value: ',len(sentence),'\\n')\n",
    "        print('Sample display value:\\n',sentence[:5])\n",
    "        return sentence\n",
    "    except FileNotFoundError:\n",
    "        print('Check and provide proper file path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s615kK3-wQda"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJfI9fhJwZk1"
   },
   "source": [
    "### Post-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ybgU2jQrwdYJ"
   },
   "outputs": [],
   "source": [
    "# A class to retrieve the sentences details from the dataframe\n",
    "class sentencedetail(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, l) for w, p, l in zip(s[\"word\"].values.tolist(), s[\"pos\"].values.tolist(),s[\"label\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ud-EcaBhwfm9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHfrXyEawf92"
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "83R3TD0Fwikg"
   },
   "outputs": [],
   "source": [
    "# Feature set\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[0]': word[0],\n",
    "        'word[-1]': word[-1],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag':postag,\n",
    "        'postag_isnounpronoun': postag in ['NOUN','PROPN'],\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word[0]': word1[0],\n",
    "            '-1:word[-1]': word1[-1],\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "            '-1:postag_isnounpronoun': postag1 in ['NOUN','PROPN']\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "            '+1:postag_isnounpronoun': postag1 in ['NOUN','PROPN']\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jjyC6LOrwmt8"
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features for a sentence.\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a2uDai49wmd_"
   },
   "outputs": [],
   "source": [
    "# Define a function to get the labels for a sentence.\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKi6r81pwrsU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OYN87gMw9GQ"
   },
   "source": [
    "# Data preprocessing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luSv6q_oxAi2"
   },
   "source": [
    "The dataset provided is in the form of one word per line. Let's understand the format of data below:\n",
    "\n",
    "Suppose there are x words in a sentence, then there will be x continuous lines with one word in each line.\n",
    "Further, the two sentences are separated by empty lines. The labels for the data follow the same format.\n",
    "We need to pre-process the data to recover the complete sentences and their labels.\n",
    "\n",
    "The above dataset is about the labels corresponding to the diseases and the treatment. There are three labels that have been used in this dataset: O, D and T, which are corresponding to Other, **Disease** and **Treatment**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w52362MJo6xO"
   },
   "source": [
    "Construct the proper sentences from individual words and print the 5 sentences.\n",
    "\n",
    "<p>\n",
    "<img src =\"https://images.upgrad.com/2a1ec8a4-e26c-4b5b-bfe0-1816d14a4a30-syntactic%20sol%20pic3.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 6.</b> </center> \n",
    " </br>  \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<img src =\"https://images.upgrad.com/cad61c6d-534f-4452-81e6-dc02d7c8fcde-syntactic%20sol%20pic5.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 7.</b> </center> \n",
    " </br>  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJFPhKH1xRKT",
    "outputId": "19f50fc3-ae89-4461-94c4-cf9d9bc36da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identified value:  2599 \n",
      "\n",
      "Sample display value:\n",
      " ['All live births > or = 23 weeks at the University of Vermont in 1995 ( n = 2395 ) were retrospectively analyzed for delivery route , indication for cesarean , gestational age , parity , and practice group ( to reflect risk status )', 'The total cesarean rate was 14.4 % ( 344 of 2395 ) , and the primary rate was 11.4 % ( 244 of 2144 )', 'Abnormal presentation was the most common indication ( 25.6 % , 88 of 344 )', \"The `` corrected '' cesarean rate ( maternal-fetal medicine and transported patients excluded ) was 12.4 % ( 273 of 2194 ) , and the `` corrected '' primary rate was 9.6 % ( 190 of 1975 )\", \"Arrest of dilation was the most common indication in both `` corrected '' subgroups ( 23.4 and 24.6 % , respectively )\"]\n"
     ]
    }
   ],
   "source": [
    "# Train sentence extraction from dataset\n",
    "train_sent = content_extract(file_path='Data/train_sent',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rcnfRAFxLat",
    "outputId": "75fab75a-75a0-440b-933b-189b2ed7719e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identified value:  2599 \n",
      "\n",
      "Sample display value:\n",
      " ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "# Train label extraction from dataset\n",
    "train_label = content_extract(file_path='Data/train_label',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0EmwgCpw-AT",
    "outputId": "bbee82e4-7e21-4518-9bb2-d8e608700d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identified value:  1056 \n",
      "\n",
      "Sample display value:\n",
      " ['Furthermore , when all deliveries were analyzed , regardless of risk status but limited to gestational age > or = 36 weeks , the rates did not change ( 12.6 % , 280 of 2214 ; primary 9.2 % , 183 of 1994 )', 'As the ambient temperature increases , there is an increase in insensible fluid loss and the potential for dehydration', 'The daily high temperature ranged from 71 to 104 degrees F and AFI values ranged from 1.7 to 24.7 cm during the study period', 'There was a significant correlation between the 2- , 3- , and 4-day mean temperature and AFI , with the 4-day mean being the most significant ( r = 0.31 , p & # 60 ; 0.001 )', 'Fluctuations in ambient temperature are inversely correlated to changes in AFI']\n"
     ]
    }
   ],
   "source": [
    "# Test sentence extraction from dataset\n",
    "test_sent = content_extract(file_path='Data/test_sent',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6mqzmHKw5XG",
    "outputId": "98334841-7f76-45bc-c63e-1454da95bb02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identified value:  1056 \n",
      "\n",
      "Sample display value:\n",
      " ['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O', 'O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "# Test label extraction from dataset\n",
    "test_label = content_extract(file_path='Data/test_label',sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjxWJafuaIkE"
   },
   "source": [
    "### Let's extract POS information using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "C4h5sBmKaFow"
   },
   "outputs": [],
   "source": [
    "# Import spacy small library to find medical related entities\n",
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-umLJDR0aFlo"
   },
   "outputs": [],
   "source": [
    "# Dataframe of POS tagging,Lemma word and Label for Train and test sentence\n",
    "train_df = pd.DataFrame(columns=['sentence','word','lemma','pos','label'])\n",
    "test_df = pd.DataFrame(columns=['sentence','word','lemma','pos','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9SXDsAypjun"
   },
   "source": [
    "Count the number of sentences, number of lines of labels in the processed train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LRjVYfjbaFiz"
   },
   "outputs": [],
   "source": [
    "#train datframe\n",
    "\n",
    "i=0 #Sentence count\n",
    "j=0 #Iteration count\n",
    "\n",
    "for sent,label in zip(train_sent,train_label):\n",
    "    i+=1\n",
    "    for s,l in zip(sent.split(),label.split()):\n",
    "        doc = nlp(s)\n",
    "        for tok in doc:\n",
    "            train_df.loc[j,['sentence','word','lemma','pos','label']] = [i,tok.text,tok.lemma_,tok.pos_,l]\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R457aBCOaFfS"
   },
   "outputs": [],
   "source": [
    "#test datframe\n",
    "\n",
    "i=0 #Sentence count\n",
    "j=0 #Iteration count\n",
    "\n",
    "for sent,label in zip(test_sent,test_label):\n",
    "    i+=1\n",
    "    for s,l in zip(sent.split(),label.split()):\n",
    "        doc = nlp(s)\n",
    "        for tok in doc:\n",
    "            test_df.loc[j,['sentence','word','lemma','pos','label']] = [i,tok.text,tok.lemma_,tok.pos_,l]\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cphGZJraU1F"
   },
   "source": [
    "**Extract those tokens which have NOUN or PROPN as their PoS tag and find their frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fj9mz4OpaUfa"
   },
   "outputs": [],
   "source": [
    "# Word and it's frequency for word which contains NOUN or PROPN as POS tagging\n",
    "freq_df = pd.DataFrame()\n",
    "freq_df = pd.concat((train_df,test_df),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DIb0Ao7AaUcj"
   },
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "freq_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhtmtTNXqAut"
   },
   "source": [
    "**Print the top 25 most common tokens with NOUN or PROPN PoS tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqNKdiyVaUZq",
    "outputId": "f45fc4fa-dd7a-4a36-da89-564c99332da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patients        492\n",
       "treatment       281\n",
       "cancer          200\n",
       "therapy         175\n",
       "disease         143\n",
       "cell            140\n",
       "lung            116\n",
       "group            94\n",
       "chemotherapy     88\n",
       "gene             88\n",
       "effects          85\n",
       "results          79\n",
       "women            77\n",
       "patient          75\n",
       "TO_SEE           75\n",
       "risk             71\n",
       "cases            71\n",
       "surgery          71\n",
       "analysis         70\n",
       "rate             67\n",
       "human            67\n",
       "response         66\n",
       "survival         65\n",
       "children         64\n",
       "effect           64\n",
       "Name: word, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 25 most frequency values for Train and Test related dataset words\n",
    "freq_df[(freq_df['pos'] == 'NOUN') | ((freq_df['pos'] == 'PROPN'))]['word'].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySkn1l99aUWx",
    "outputId": "3a45544b-9c60-4768-c242-ee26b0f6da83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient         587\n",
       "treatment       316\n",
       "cancer          226\n",
       "cell            203\n",
       "therapy         182\n",
       "disease         172\n",
       "effect          163\n",
       "case            132\n",
       "group           128\n",
       "lung            120\n",
       "result          118\n",
       "gene            112\n",
       "year            105\n",
       "rate            102\n",
       "trial            91\n",
       "chemotherapy     91\n",
       "woman            89\n",
       "analysis         86\n",
       "protein          82\n",
       "response         81\n",
       "child            78\n",
       "risk             78\n",
       "human            77\n",
       "mutation         75\n",
       "TO_SEE           75\n",
       "Name: lemma, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 25 most frequency values for Train and Test related lemma words\n",
    "freq_df[(freq_df['pos'] == 'NOUN') | ((freq_df['pos'] == 'PROPN'))]['lemma'].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcSe8Uo8aoW3"
   },
   "source": [
    "**Dataframe (Sentence, word, POS) visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "D97tmW6naoD-",
    "outputId": "0485d8e9-9819-4205-d66e-e7592d08c32f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>All</td>\n",
       "      <td>all</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>live</td>\n",
       "      <td>live</td>\n",
       "      <td>VERB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>births</td>\n",
       "      <td>birth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence    word  lemma    pos label\n",
       "0        1     All    all   PRON     O\n",
       "1        1    live   live   VERB     O\n",
       "2        1  births  birth   NOUN     O\n",
       "3        1       >      >  PUNCT     O\n",
       "4        1      or     or  CCONJ     O"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jKvwWbOqaoAn",
    "outputId": "94b962cb-9b6a-49c9-9d24-31a2eae703a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Furthermore</td>\n",
       "      <td>furthermore</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>deliveries</td>\n",
       "      <td>delivery</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence         word        lemma    pos label\n",
       "0        1  Furthermore  furthermore    ADV     O\n",
       "1        1            ,            ,  PUNCT     O\n",
       "2        1         when         when  SCONJ     O\n",
       "3        1          all          all   PRON     O\n",
       "4        1   deliveries     delivery   NOUN     O"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1dbrjJ5awcJ"
   },
   "source": [
    "**Sentense-wise detail dataframe preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "15OFlzUran9v"
   },
   "outputs": [],
   "source": [
    "# Fetch detail view of sentence for train set\n",
    "train_sent_obj = sentencedetail(train_df)\n",
    "train_sent_detail = train_sent_obj.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWythcLFan65",
    "outputId": "dd0db094-6e07-4554-e5a2-22d7b122b99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('All', 'PRON', 'O'),\n",
       " ('live', 'VERB', 'O'),\n",
       " ('births', 'NOUN', 'O'),\n",
       " ('>', 'PUNCT', 'O'),\n",
       " ('or', 'CCONJ', 'O'),\n",
       " ('=', 'VERB', 'O'),\n",
       " ('23', 'NUM', 'O'),\n",
       " ('weeks', 'NOUN', 'O'),\n",
       " ('at', 'ADP', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('University', 'NOUN', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('Vermont', 'PROPN', 'O'),\n",
       " ('in', 'ADP', 'O'),\n",
       " ('1995', 'NUM', 'O'),\n",
       " ('(', 'PUNCT', 'O'),\n",
       " ('n', 'CCONJ', 'O'),\n",
       " ('=', 'VERB', 'O'),\n",
       " ('2395', 'NUM', 'O'),\n",
       " (')', 'PUNCT', 'O'),\n",
       " ('were', 'AUX', 'O'),\n",
       " ('retrospectively', 'ADV', 'O'),\n",
       " ('analyzed', 'VERB', 'O'),\n",
       " ('for', 'ADP', 'O'),\n",
       " ('delivery', 'NOUN', 'O'),\n",
       " ('route', 'NOUN', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('indication', 'NOUN', 'O'),\n",
       " ('for', 'ADP', 'O'),\n",
       " ('cesarean', 'VERB', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('gestational', 'ADJ', 'O'),\n",
       " ('age', 'NOUN', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('parity', 'NOUN', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('and', 'CCONJ', 'O'),\n",
       " ('practice', 'VERB', 'O'),\n",
       " ('group', 'NOUN', 'O'),\n",
       " ('(', 'PUNCT', 'O'),\n",
       " ('to', 'PART', 'O'),\n",
       " ('reflect', 'VERB', 'O'),\n",
       " ('risk', 'NOUN', 'O'),\n",
       " ('status', 'NOUN', 'O'),\n",
       " (')', 'PUNCT', 'O')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display one sentence detail view for train set\n",
    "train_sent_detail[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ssd2OuO9an3_"
   },
   "outputs": [],
   "source": [
    "# Fetch detail view of sentence for train set\n",
    "test_sent_obj = sentencedetail(test_df)\n",
    "test_sent_detail = test_sent_obj.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5COsnv4an1I",
    "outputId": "01ce10d3-a845-4546-df09-6d2b08623976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furthermore', 'ADV', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('when', 'SCONJ', 'O'),\n",
       " ('all', 'PRON', 'O'),\n",
       " ('deliveries', 'NOUN', 'O'),\n",
       " ('were', 'AUX', 'O'),\n",
       " ('analyzed', 'VERB', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('regardless', 'ADV', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('risk', 'NOUN', 'O'),\n",
       " ('status', 'NOUN', 'O'),\n",
       " ('but', 'CCONJ', 'O'),\n",
       " ('limited', 'VERB', 'O'),\n",
       " ('to', 'PART', 'O'),\n",
       " ('gestational', 'ADJ', 'O'),\n",
       " ('age', 'NOUN', 'O'),\n",
       " ('>', 'PUNCT', 'O'),\n",
       " ('or', 'CCONJ', 'O'),\n",
       " ('=', 'VERB', 'O'),\n",
       " ('36', 'NUM', 'O'),\n",
       " ('weeks', 'NOUN', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('rates', 'NOUN', 'O'),\n",
       " ('did', 'VERB', 'O'),\n",
       " ('not', 'PART', 'O'),\n",
       " ('change', 'VERB', 'O'),\n",
       " ('(', 'PUNCT', 'O'),\n",
       " ('12.6', 'NUM', 'O'),\n",
       " ('%', 'INTJ', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('280', 'NUM', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('2214', 'NUM', 'O'),\n",
       " (';', 'PUNCT', 'O'),\n",
       " ('primary', 'NOUN', 'O'),\n",
       " ('9.2', 'NUM', 'O'),\n",
       " ('%', 'INTJ', 'O'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('183', 'NUM', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('1994', 'NUM', 'O'),\n",
       " (')', 'PUNCT', 'O')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display one sentence detail view for train set\n",
    "test_sent_detail[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnIqDnfOa_Y2"
   },
   "source": [
    "# Define input and target variables\n",
    "\n",
    "Correctly computing X and Y sequence matrices for training and test data. Check that both sentences and labels are processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTn9KNmFqXHP"
   },
   "source": [
    "Define the features' values for each sentence as input variable for CRF model in test and the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qtwXc97la_Ix"
   },
   "outputs": [],
   "source": [
    "# Prepare X-train and X-test by extracting features from train and test dataset\n",
    "X_train = [sent2features(s) for s in train_sent_detail]\n",
    "X_test = [sent2features(s) for s in test_sent_detail]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZPL_88uqaOs"
   },
   "source": [
    "Define the labels as the target variable for test and the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iGDyJo8Ea_GE"
   },
   "outputs": [],
   "source": [
    "# Prepare y-train and y-test by extracting labels from train and test dataset\n",
    "y_train = [sent2labels(l) for l in train_sent_detail]\n",
    "y_test = [sent2labels(l) for l in test_sent_detail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1TgP3Rqa_De"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKIvoPWvbGCk"
   },
   "source": [
    "# Building the CRF Model using sklearn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "y2WgdG60hIYG"
   },
   "outputs": [],
   "source": [
    "# Import model and metrics\n",
    "from sklearn_crfsuite import CRF, scorers, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkoIcgAlqnuN"
   },
   "source": [
    "Predict the labels of each of the tokens in each sentence of the test dataset that has been pre processed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnjjZaTNa_Am",
    "outputId": "4c1b5d90-d5b2-40ec-ea0d-9cbe76bbfac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.46 s, sys: 61.4 ms, total: 4.52 s\n",
      "Wall time: 4.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build the CRF model.\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umxl5Md1a--L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07bFEgKCbKHx"
   },
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en-4QbmkqrEG"
   },
   "source": [
    "Calculate the f1 score using the actual labels and the predicted labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AdsLFm9a-7K",
    "outputId": "8726120d-f08b-49ee-bc4e-bcb72d5d17d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted F1-score for Medical Entity Dataset is: 92.5 % \n"
     ]
    }
   ],
   "source": [
    "# Calculate the f1 score using the test data\n",
    "# y_pred = crf.predict(X_test)\n",
    "\n",
    "f1_score = metrics.flat_f1_score(y_test, y_pred, average='weighted')\n",
    "print('Predicted F1-score for Medical Entity Dataset is: {0} % '.format(round(f1_score*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu6j9cmua-4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P5eyHmHbT4O"
   },
   "source": [
    "# Predict Disease and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zP4NaSi6aUUH"
   },
   "outputs": [],
   "source": [
    "# Taken out predicted label from the model\n",
    "pred_label=[]\n",
    "for i in y_pred:\n",
    "    pred_label.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bxUyiR-vaURI"
   },
   "outputs": [],
   "source": [
    "# Loaded into test dataframe\n",
    "test_df['label_predicted'] = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lj5WIg1_aUN_",
    "outputId": "8f16dc6b-65a0-42ae-b43d-e468860118d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "      <th>label_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Furthermore</td>\n",
       "      <td>furthermore</td>\n",
       "      <td>ADV</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PRON</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>deliveries</td>\n",
       "      <td>delivery</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence         word        lemma    pos label label_predicted\n",
       "0        1  Furthermore  furthermore    ADV     O               O\n",
       "1        1            ,            ,  PUNCT     O               O\n",
       "2        1         when         when  SCONJ     O               O\n",
       "3        1          all          all   PRON     O               O\n",
       "4        1   deliveries     delivery   NOUN     O               O"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise top 5 data\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr6oNjjAaFcZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDFez-vabc07"
   },
   "source": [
    "##Identifying Diseases and Treatments using Custom NER\n",
    "We now use the CRF model's prediction to prepare a record of diseases identified in the corpus and treatments used for the diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaInwBEGq38C"
   },
   "source": [
    "**Create the logic to get all the predicted treatments (T) labels corresponding to each disease (D) label in the test dataset.**\n",
    "\n",
    "<p>\n",
    "<img src =\"https://images.upgrad.com/0891d77b-b9ca-4e9d-8934-d9a9b078a51c-syntactic%20sol%20pic1.png\" alt='Figure 1'>\n",
    "<center> <b>Figure 8.</b> </center> \n",
    " </br>  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2zb1s18OberF"
   },
   "outputs": [],
   "source": [
    "# Preparing dictionary by keeping Disease as unique Key element and Treatment as value element\n",
    "new_df =test_df[(test_df['label_predicted'] != 'O')]\n",
    "new_df.set_index('sentence',inplace=True)\n",
    "disease=[]\n",
    "treatment=[]\n",
    "sentence=[]\n",
    "med_dict = {}\n",
    "for i in new_df.index.unique():\n",
    "    try:\n",
    "        val = new_df.loc[i,'label_predicted'].unique()\n",
    "        if len(val) == 2:\n",
    "            disease_val = new_df[new_df['label_predicted'] == 'D'].loc[i,'word']\n",
    "            treatment_val = new_df[new_df['label_predicted'] == 'T'].loc[i,'word']\n",
    "            disease_single = disease_val if type(disease_val) == str else \" \".join(disease_val)\n",
    "            treatment_single = treatment_val if type(treatment_val) == str else \" \".join(treatment_val)\n",
    "            if disease_single not in disease:\n",
    "                med_dict[disease_single] = treatment_single\n",
    "            else:\n",
    "                print('Entered')\n",
    "                med_dict[disease_single] = med_dict.get(disease_single)+'/'+treatment_single\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rlswx9GmbiM5",
    "outputId": "829fe1a3-3c90-4743-928b-49425ddbf85c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macrosomic infants in gestational diabetes cases': 'good glycemic control', 'nonimmune hydrops fetalis': 'Trisomy', 'retinoblastoma': 'radiotherapy', 'epilepsy': 'Methylphenidate', 'unstable angina or non - Q - wave myocardial infarction': 'roxithromycin', 'coronary - artery disease': 'Antichlamydial antibiotics', 'primary pulmonary hypertension ( PPH )': 'fenfluramines', 'essential hypertension': 'moxonidine `', 'cellulitis': 'G - CSF therapy intravenous antibiotic treatment', 'foot infection in diabetic patients': 'G - CSF treatment', 'hemorrhagic stroke': 'double - bolus alteplase accelerated infusion of alteplase ( P=0.24', 'cardiac disease': 'fenfluramine - phentermine', 'rheumatoid arthritis': 'arthrodesis', \"early Parkinson 's disease\": 'Ropinirole monotherapy', 'sore throat': 'Antibiotics', \"Crohn 's disease\": 'steroids', 'stress urinary incontinence': 'surgical procedures', 'female stress urinary incontinence': 'surgical treatment', 'preeclampsia ( proteinuric hypertension )': 'intrauterine insemination with donor sperm intrauterine insemination', 'mild preeclampsia preeclampsia': 'partner insemination program donor insemination program', 'hyperammonemia cancer': 'organ transplantation and chemotherapy', 'major pulmonary embolism': 'thrombolytic treatment', 'malignant pleural mesothelioma': 'thoracotomy , radiotherapy , and chemotherapy', 'tumor necrosis TSS': 'interferon - gamma', 'pulmonary symptoms mediastinal teratomas': 'chemotherapy', 'non - obstructive azoospermia': 'TEFNA', 'testicular bleeding': 'fine needle aspiration', 'heart failure': 'beta - blockade', 'acute migraine treatment': 'Sumatriptan', 'duodenal ulcer': 'Measured subtotal gastrectomy', 'colorectal cancer': 'docosahexanoic acid ( DHA ) -concentrated fish oil capsules', 'gastrointestinal tumours': 'Elective surgery', 'primary pulmonary hypertension': 'fenfluramine', 'restenosis': 'coronary angioplasty', \"oral Crohn 's disease\": 'Thalidomide', 'bronchial asthma': 'Non - steroidal anti - inflammatory therapy', 'splenic rupture': 'granulocyte colony - stimulating factor ( G - CSF )', \"Parkinson 's disease\": 'Microelectrode - guided posteroventral pallidotomy', 'Alopecia': 'tacrolimus therapy', 'cardiac insufficiency': 'beta - blockers', 'Cranial nerve injuries': 'persistent conduction blocks', \"Eisenmenger 's syndrome\": 'laparoscopic cholecystectomy', 'breast cancer': 'subcutaneous mastectomy', 'severe psoriasis': 'systemic treatments', 'non - seminomatous germ - cell tumors': 'chemotherapy', 'pre - established metastases': 'adenovirus - mediated interleukin 12 ( il-12 ) gene therapy', 'malignant tumors such as non - small cell lung cancer': 'surgery', 'inflammatory skin diseases': 'topical corticosteroids', 'cancer unresectable stage iii nsclc': 'sequential chemotherapy followed by radiation', 'nsclc nsclc ( stage iiib ) sclc': 'got surgical treatment radiotherapy', 'nsclc': 'chemotherapy', 'bos': 'concurrent acute rejection , therapy with extracorporeal photopheresis', 'locally advanced non - small - cell lung cancer ( la - nsclc )': 'combined - modality therapy ( cmt ; chemotherapy and radiotherapy )', 'radiation - induced myelopathy': 'heparin and enoxaparin', 'regionally advanced disease': 'resection , allowing neoadjuvant therapy', 'malignant pleural effusions from nsclc': 'systemic chemotherapy', 'supraclavicular node metastases in nsclc': 'chemoradiotherapy', 'non - small - cell - lung - cancer ( nsclc )': 'cisplatin and radiotherapy', 'lung carcinoma': 'curative therapy', 'single non - sclc melanoma ovarian carcinoma brain metastasis': 'surgical resection', 'colorectal metastases': 'therapeutic vats metastasectomy', 'limited - stage small - cell carcinoma of the lung': 'combined - modality therapy', 'advanced nsclc': 'combination chemotherapy of cisplatin , ifosfamide and irinotecan with rhg - csf support', 'metastatic colorectal cancer': 'intravenous oxaliplatin', \"non - hodgkin 's lymphoma breast cancer mesothelioma and non - small cell lung cancer\": 'oxaliplatin', 'primary tumor ( li ) bronchogenic carcinoma': 'resection', 'non - small cell lung cancer hormone refractory prostate cancer': 'paclitaxel and carboplatin', 'primary lung cancer adenocarcinoma ( ad ) squamous cell carcinoma': 'resection', 'stage iii nsclc': 'chemotherapy surgery', 'small - cell lung cancer': 'combination chemotherapy', 'symptomatic metastases': 'radiotherapy', 'primary cancer': 'adjuvant radiation therapy', 'advanced non -- small - cell lung cancer': 'paclitaxel plus carboplatin ( pc ) vinorelbine plus cisplatin ( vc )', 'sclc extensive disease': 'platinum dose ( cisplatin plus carboplatin ) in combination chemotherapy combination therapy with carboplatin', 'untreated small cell lung cancer ( sclc ) untreated sclc': 'technetium-99 m tetrofosmin ( tc - tf ) accumulation chemotherapy', 'neutropenia': 'recombinant human granulocyte colony - stimulating factor ( rhg - csf ) cancer chemotherapy', 'head and neck cancer xerostomia': 'irradiation therapy', 'psoriasis': 'topical therapy', 'disseminated malignant melanoma': 'leukocyte A recombinant interferon ( rIFN - alpha A , Roferon - A', 'advanced stage ( TNM IIB - IVB ) mycosis fungoides': 'combination chemotherapy program consisting of bleomycin and methotrexate weekly', 'ventricular tachycardia': 'surgical therapy', 'non - functioning endocrine pancreatic tumor with progressive liver and lymph node metastases': 'i.m . lanreotide therapy', 'cholestasis': 'Inchinko - to ( TJ-135 ) crude drugs', 'severe acute hepatitis accompanying cholestasis autoimmune hepatitis': 'TJ-135', 'syringomyelia spinal adhesive arachnoiditis': 'Surgical management', 'symptomatic bronchiectasis': 'antibiotics , antibronchoobstructive medication', 'bronchiectasis': 'Current surgical therapy', 'biliary colic symptoms biliary dyskinesia': 'cholecystectomy', 'biliary dyskinesia': 'Cholecystectomy', 'paranasal sinuses common cold': 'pseudoephedrine plus acetaminophen', 'inflammation': 'video - assisted thoracoscopic surgery', 'Pneumocystis carinii pneumonia': 'trimethoprim - sulfamethoxazole', 'acute nasopharyngitis': 'antibiotic treatment', 'common cold': 'Macrolide antibiotics', 'infection': 'antileukemic therapy', 'pre - antibiotic era multidrug resistant TB': 'Therapeutic pneumothorax ( TP', 'influenza breast cancer': 'vaccination', 'mouse - adopted strain of influenza A2 ( H2N2 ) virus': 'Gingyo - san', 'carcinoma': 'esophagectomy', 'persistent asthma': 'corticosteroids', 'asthma': 'Fluticasone propionate', 'chronic hepatitis C': 'Combination therapy with interferon - alpha ( IFN alpha ) plus Ribavirin', 'hepatitis C viremia': 'combination therapy', 'duodenogastric reflux': 'cholecystectomy', 'severe hypoxemia': 'mechanical ventilation glucocorticoid pulse therapy', 'AOM drug - resistant S. pneumoniae': 'Amoxicillin remains the antibiotic of choice', 'bacterial meningitis Haemophilus influenzae meningitis': 'antibiotic - resistant strains', 'bacterial meningitis': 'vaccines', 'macular degeneration ( AMD )': 'external beam radiation therapy', 'acute myocardial infarction': 'thrombolytic treatment', 'depression hyponatremia': 'venlafaxine', 'ischemic heart disease': 'Aortocoronary bypass grafting', 's.c . tumors peritoneal tumors': 'Subcutaneous injection of irradiated LLC - IL2 did not', 'acute occlusion of the middle cerebral artery large embolus or high - grade stenosis': 'thrombolytic therapy', 'autoimmune diseases': 'High - dose intravenous immunoglobulin ( hdIVIg )', 'cancer': 'Matrix metalloproteinase inhibitors', 'large - bowel cancer': 'oral UFT', 'phaeochromocytoma': 'Adrenalectomy', 'malignant melanoma': 'single agent therapy interferon alfa-2a', 'advanced renal cell carcinoma': 'various interferon alpha preparations interferon alfa - N1 , interferon alfa-2a interferon alfa-2b', \"low - grade non - Hodgkin 's lymphoma\": 'interferon alpha', \"low - grade non - Hodgkin 's lymphomas\": 'interferon interferon', 'partial seizures': 'lamotrigine monotherapy', 'esophageal achalasia': 'botulinum toxin injection , pneumatic dilation laparoscopic myotomy', 'abnormal uterine bleeding': 'therapeutic hysteroscopy', 'irritable bowel syndrome': 'Chinese herbal medicine', 'proximal hypospadias': 'Tubularized incised plate hypospadias repair', 'prostate cancer': 'radical prostatectomy and iodine 125 interstitial radiotherapy', 'sickle cell disease': 'hydroxyurea', 'stroke': 'Statins', 'mitomycin - resistant bladder cancer': 'photodynamic therapy in combination with mitomycin C', 'B16 melanoma': 'adenosine triphosphate and treatment with buthionine sulfoximine', 'advanced rectal cancer': 'Nerve - sparing surgery', 'spontaneous pneumothorax': 'Thoracoscopic surgery', 'empyema': 'Thoracoscopy', 'acute cerebral ischemia': 'Antiplatelet therapy', 'renal cell carcinoma': 'Interferon treatment', \"Barrett 's esophagus\": 'Acid suppression therapy', 'autoimmune hemolytic anemia': 'heparin', 'ovarian cancer': 'chemotherapy', 'stroke brain hemorrhage atrial fibrillation': 'Antiplatelet therapy', 'lymphoma': 'Paclitaxel', 'renovascular hypertension': 'Percutaneous transluminal angioplasty', 'moderately symptomatic benign prostatic hyperplasia': 'surgical resection', 'pulmonary hypertension': 'Single or double lung transplantation', 'multiple sclerosis': 'Intravenous immunoglobulin treatment', 'acoustic neuroma': 'Stereotactic radiosurgery', 'cerebral palsy': 'Hyperbaric oxygen therapy', 'postvitrectomy diabetic vitreous hemorrhage': 'Peripheral retinal cryotherapy', 'acute carbon monoxide poisoning': 'Hyperbaric or normobaric oxygen', 'hepatitis B': 'vaccine', 'pertussis': 'vaccines', 'migraine': 'sumatriptan', 'perioperative mortality and myocardial infarction': 'vascular surgery', 'severe secondary peritonitis': 'Surgical management', 'hepatic metastases from colorectal cancer': 'Hepatic arterial infusion of chemotherapy', 'epithelial ovarian cancer': 'High - dose chemotherapy with autologous stem - cell support', 'multiple myeloma': 'chemoradiotherapy with autologous stem - cell support'}\n"
     ]
    }
   ],
   "source": [
    "print(med_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-8Iyuz5biKS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4M3bPJ2rQSg"
   },
   "source": [
    "**Predict the treatment for the disease name: 'hereditary retinoblastoma'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PaobAzWbiHr",
    "outputId": "33320dde-e9e5-40f3-d8f2-6a684f4115b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Disease:  retinoblastoma\n",
      "Identified Treatment:  radiotherapy\n"
     ]
    }
   ],
   "source": [
    "#Predict treatment withthe help of dictionary\n",
    "d=[]\n",
    "disease=''\n",
    "test_sent=[]\n",
    "treatment=''\n",
    "\n",
    "input_sent = 'hereditary retinoblastoma'\n",
    "m = spacy.load('en_core_web_sm')\n",
    "doc = m(input_sent)\n",
    "for i in doc:\n",
    "    d.append((i.text,i.pos_,'D'))\n",
    "test_sent.append(sent2features(d))\n",
    "for i,tag in enumerate(crf.predict(test_sent)[0]):\n",
    "    if tag == 'D':\n",
    "        tr = input_sent.split()[i]\n",
    "        disease += tr\n",
    "        if tr in med_dict:\n",
    "            treatment += ''+med_dict.get(tr)\n",
    "if len(treatment) == 0:\n",
    "    treatment='None'\n",
    "print('Identified Disease: ',disease)\n",
    "print('Identified Treatment: ', treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZL2Yx7fbiE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOieBS8cbh9V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYLj8Pbobh7Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MgVHMzQbh3q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbBkpUa9bh08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
